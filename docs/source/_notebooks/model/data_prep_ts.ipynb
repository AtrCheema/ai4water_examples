{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AtrCheema/ai4water_examples/blob/master/docs/source/_notebooks/model/data_prep_ts.ipynb)\n",
    "\n",
    "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/AtrCheema/ai4water_examples/blob/master/docs/source/_notebooks/model/data_prep_ts.ipynb)\n",
    " \n",
    "\n",
    "## Data Preparation for Timeseries forecasting\n",
    "\n",
    "This notebook describes how to prepare data for RNN/LSTM for timeseries prediction problem especially for Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Srtqrlwr09Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "Create a data which is supposed to represent a timeseries prediction problem. The data has 6 columns and 1000 rows. The first five columns are supposed to be input and the last column is supposed to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  2000  4000  6000  8000 10000]\n",
      " [    1  2001  4001  6001  8001 10001]\n",
      " [    2  2002  4002  6002  8002 10002]\n",
      " [    3  2003  4003  6003  8003 10003]\n",
      " [    4  2004  4004  6004  8004 10004]\n",
      " [    5  2005  4005  6005  8005 10005]\n",
      " [    6  2006  4006  6006  8006 10006]\n",
      " [    7  2007  4007  6007  8007 10007]\n",
      " [    8  2008  4008  6008  8008 10008]\n",
      " [    9  2009  4009  6009  8009 10009]\n",
      " [   10  2010  4010  6010  8010 10010]\n",
      " [   11  2011  4011  6011  8011 10011]\n",
      " [   12  2012  4012  6012  8012 10012]\n",
      " [   13  2013  4013  6013  8013 10013]\n",
      " [   14  2014  4014  6014  8014 10014]\n",
      " [   15  2015  4015  6015  8015 10015]\n",
      " [   16  2016  4016  6016  8016 10016]\n",
      " [   17  2017  4017  6017  8017 10017]\n",
      " [   18  2018  4018  6018  8018 10018]\n",
      " [   19  2019  4019  6019  8019 10019]]\n",
      "\n",
      " (2000, 6) \n",
      "\n",
      "[[ 1980  3980  5980  7980  9980 11980]\n",
      " [ 1981  3981  5981  7981  9981 11981]\n",
      " [ 1982  3982  5982  7982  9982 11982]\n",
      " [ 1983  3983  5983  7983  9983 11983]\n",
      " [ 1984  3984  5984  7984  9984 11984]\n",
      " [ 1985  3985  5985  7985  9985 11985]\n",
      " [ 1986  3986  5986  7986  9986 11986]\n",
      " [ 1987  3987  5987  7987  9987 11987]\n",
      " [ 1988  3988  5988  7988  9988 11988]\n",
      " [ 1989  3989  5989  7989  9989 11989]\n",
      " [ 1990  3990  5990  7990  9990 11990]\n",
      " [ 1991  3991  5991  7991  9991 11991]\n",
      " [ 1992  3992  5992  7992  9992 11992]\n",
      " [ 1993  3993  5993  7993  9993 11993]\n",
      " [ 1994  3994  5994  7994  9994 11994]\n",
      " [ 1995  3995  5995  7995  9995 11995]\n",
      " [ 1996  3996  5996  7996  9996 11996]\n",
      " [ 1997  3997  5997  7997  9997 11997]\n",
      " [ 1998  3998  5998  7998  9998 11998]\n",
      " [ 1999  3999  5999  7999  9999 11999]]\n"
     ]
    }
   ],
   "source": [
    "rows = 2000\n",
    "cols = 6\n",
    "data = np.arange(int(rows*cols)).reshape(-1,rows).transpose()\n",
    "print(data[0:20])  \n",
    "print('\\n {} \\n'.format(data.shape))\n",
    "print(data[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (581, 7, 5) \n",
      "shape of y data: (587, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 581\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 576\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 581] \n",
      "\n",
      "Number of batches are 36 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n"
     ]
    }
   ],
   "source": [
    "def first_nan_from_end(ar):\n",
    "    \"\"\" \n",
    "    This function finds index for first nan from the group which is present at the end of input array `ar`.\n",
    "    Some examples are below\n",
    "    [np.nan, np.nan, 0, 2, 3, 0, 3, np.nan, np.nan, np.nan, np.nan] >> 7\n",
    "    [np.nan, np.nan, 1, 2, 3, 0, np.nan, np.nan, np.nan] >> 6\n",
    "    [0, 2, 3, 0, 3] >> 5\n",
    "    [np.nan, np.nan, 0,2,3,0,3] >> 7    \n",
    "    \"\"\"\n",
    "    last_non_zero=0\n",
    "    \n",
    "    for idx, val in enumerate(ar[::-1]):  # first find first non-nan value starting from last\n",
    "        if ~np.isnan(val): # val >= 0:\n",
    "            last_non_zero = idx\n",
    "            break\n",
    "    return ar.shape[0] - last_non_zero    \n",
    "    \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=None, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    # normalizting both x and y data\n",
    "    if norm:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)        \n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "    \n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step                 # starting point of sampling from data may not start from 0 \n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans or nans which are present at the end part of array\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:                              # if some values from start were skipped, we need to remove nans from those places\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]\n",
    "\n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]  # ptential samples\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    if residue > 0:\n",
    "        interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "    \n",
    "    # The last batch may not fewer data as other batches. We can skip that incomplete batch.\n",
    "    # This can be useful if want to save our batches in a list i.e. if 'x_batches' is a list.\n",
    "    if trim_last_batch:   #TODO this must be obligatory when saving batches in numpy array\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "        \n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "\n",
    "    # container for batches\n",
    "    x_batches = np.full((no_of_batches, batch_size, lookback, in_features), np.nan)\n",
    "    y_batches = np.full((no_of_batches, batch_size, out_features), np.nan)\n",
    "\n",
    "\n",
    "    for b in range(no_of_batches):\n",
    "        st = interval[b]\n",
    "        en = interval[b + 1]\n",
    "        an_x_batch = x_wins[st:en, :, :]\n",
    "        x_batches[b] = an_x_batch\n",
    "       # y_batches[b] = y_wins[st:en]\n",
    "        y_batches[b] = y_wins[st+1:en+1]\n",
    "\n",
    "\n",
    "    print('\\nshape of batches for:')\n",
    "    print('x_data ', ' y_data')\n",
    "    for i,j in zip(x_batches, y_batches):\n",
    "        ishp, jshp = None, None\n",
    "        if isinstance(i, np.ndarray):\n",
    "            ishp = i.shape\n",
    "        if isinstance(j, np.ndarray):\n",
    "            jshp = j.shape\n",
    "        print(ishp, jshp)\n",
    "    \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=7  # sequence length\n",
    "input_features = 5  # number of columns in dataset to be used as input\n",
    "output_features = 1 # number of column to be used as output\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_x_batches, train_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    trim_last_batch = True)            \n",
    "\n",
    "test_x_batches, test_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                                    min_ind = 600,\n",
    "                                    max_ind = 800,\n",
    "                                    future_y_val = t_plus_ith_val,\n",
    "                                    trim_last_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first train batch\n",
    "We can look at first training batch which consist of 16 windows where each window has shape [7, 5]. One training window is fed at one time step and each window has correspoding prediction.\n",
    "Also look where each window starts and ends and the `y` values for each window.\n",
    "Values in rows of windows are not continuous because o f `input_stepsize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 2000. 4000. 6000. 8000.]\n",
      " [   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]] [10014.] \n",
      "\n",
      "[[   1. 2001. 4001. 6001. 8001.]\n",
      " [   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]] [10015.] \n",
      "\n",
      "[[   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]] [10016.] \n",
      "\n",
      "[[   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]] [10017.] \n",
      "\n",
      "[[   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]] [10018.] \n",
      "\n",
      "[[   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]] [10019.] \n",
      "\n",
      "[[   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]] [10020.] \n",
      "\n",
      "[[   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]] [10021.] \n",
      "\n",
      "[[   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]] [10022.] \n",
      "\n",
      "[[   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]] [10023.] \n",
      "\n",
      "[[  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]] [10024.] \n",
      "\n",
      "[[  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]] [10025.] \n",
      "\n",
      "[[  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]] [10026.] \n",
      "\n",
      "[[  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]] [10027.] \n",
      "\n",
      "[[  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]] [10028.] \n",
      "\n",
      "[[  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]] [10029.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[0], train_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to skip certain values from start because of `lookup` size\n",
    "#### second train batch\n",
    "Windows in second training batch start from exactly where the windows in first ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]] [10030.] \n",
      "\n",
      "[[  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]] [10031.] \n",
      "\n",
      "[[  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]] [10032.] \n",
      "\n",
      "[[  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]] [10033.] \n",
      "\n",
      "[[  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]] [10034.] \n",
      "\n",
      "[[  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]] [10035.] \n",
      "\n",
      "[[  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]] [10036.] \n",
      "\n",
      "[[  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]] [10037.] \n",
      "\n",
      "[[  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]] [10038.] \n",
      "\n",
      "[[  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]] [10039.] \n",
      "\n",
      "[[  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]] [10040.] \n",
      "\n",
      "[[  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]] [10041.] \n",
      "\n",
      "[[  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]] [10042.] \n",
      "\n",
      "[[  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]] [10043.] \n",
      "\n",
      "[[  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]\n",
      " [  42. 2042. 4042. 6042. 8042.]] [10044.] \n",
      "\n",
      "[[  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]\n",
      " [  43. 2043. 4043. 6043. 8043.]] [10045.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[1], train_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last train batch\n",
    "Some values from end are also missing because we skipped the last batch which did not have the shape of [16, 7, 5]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 560. 2560. 4560. 6560. 8560.]\n",
      " [ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]] [10574.] \n",
      "\n",
      "[[ 561. 2561. 4561. 6561. 8561.]\n",
      " [ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]] [10575.] \n",
      "\n",
      "[[ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]] [10576.] \n",
      "\n",
      "[[ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]] [10577.] \n",
      "\n",
      "[[ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]] [10578.] \n",
      "\n",
      "[[ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]] [10579.] \n",
      "\n",
      "[[ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]] [10580.] \n",
      "\n",
      "[[ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]] [10581.] \n",
      "\n",
      "[[ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]] [10582.] \n",
      "\n",
      "[[ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]] [10583.] \n",
      "\n",
      "[[ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]] [10584.] \n",
      "\n",
      "[[ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]] [10585.] \n",
      "\n",
      "[[ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]] [10586.] \n",
      "\n",
      "[[ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]] [10587.] \n",
      "\n",
      "[[ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]\n",
      " [ 586. 2586. 4586. 6586. 8586.]] [10588.] \n",
      "\n",
      "[[ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]\n",
      " [ 587. 2587. 4587. 6587. 8587.]] [10589.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[-1], train_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first test batch\n",
    "The start of test data depends upon the value of `min_ind` and `max_ind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600. 2600. 4600. 6600. 8600.]\n",
      " [ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]] [10614.] \n",
      "\n",
      "[[ 601. 2601. 4601. 6601. 8601.]\n",
      " [ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]] [10615.] \n",
      "\n",
      "[[ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]] [10616.] \n",
      "\n",
      "[[ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]] [10617.] \n",
      "\n",
      "[[ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]] [10618.] \n",
      "\n",
      "[[ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]] [10619.] \n",
      "\n",
      "[[ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]] [10620.] \n",
      "\n",
      "[[ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]] [10621.] \n",
      "\n",
      "[[ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]] [10622.] \n",
      "\n",
      "[[ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]] [10623.] \n",
      "\n",
      "[[ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]] [10624.] \n",
      "\n",
      "[[ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]] [10625.] \n",
      "\n",
      "[[ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]] [10626.] \n",
      "\n",
      "[[ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]] [10627.] \n",
      "\n",
      "[[ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]] [10628.] \n",
      "\n",
      "[[ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]] [10629.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[0], test_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]] [10630.] \n",
      "\n",
      "[[ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]] [10631.] \n",
      "\n",
      "[[ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]] [10632.] \n",
      "\n",
      "[[ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]] [10633.] \n",
      "\n",
      "[[ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]] [10634.] \n",
      "\n",
      "[[ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]] [10635.] \n",
      "\n",
      "[[ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]] [10636.] \n",
      "\n",
      "[[ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]] [10637.] \n",
      "\n",
      "[[ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]] [10638.] \n",
      "\n",
      "[[ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]] [10639.] \n",
      "\n",
      "[[ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]] [10640.] \n",
      "\n",
      "[[ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]] [10641.] \n",
      "\n",
      "[[ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]] [10642.] \n",
      "\n",
      "[[ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]] [10643.] \n",
      "\n",
      "[[ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]\n",
      " [ 642. 2642. 4642. 6642. 8642.]] [10644.] \n",
      "\n",
      "[[ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]\n",
      " [ 643. 2643. 4643. 6643. 8643.]] [10645.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[1], test_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### second last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 744. 2744. 4744. 6744. 8744.]\n",
      " [ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]] [10758.] \n",
      "\n",
      "[[ 745. 2745. 4745. 6745. 8745.]\n",
      " [ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]] [10759.] \n",
      "\n",
      "[[ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]] [10760.] \n",
      "\n",
      "[[ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]] [10761.] \n",
      "\n",
      "[[ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]] [10762.] \n",
      "\n",
      "[[ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]] [10763.] \n",
      "\n",
      "[[ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]] [10764.] \n",
      "\n",
      "[[ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]] [10765.] \n",
      "\n",
      "[[ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]] [10766.] \n",
      "\n",
      "[[ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]] [10767.] \n",
      "\n",
      "[[ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]] [10768.] \n",
      "\n",
      "[[ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]] [10769.] \n",
      "\n",
      "[[ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]] [10770.] \n",
      "\n",
      "[[ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]] [10771.] \n",
      "\n",
      "[[ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]] [10772.] \n",
      "\n",
      "[[ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]] [10773.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-2], test_y_batches[-2]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]] [10774.] \n",
      "\n",
      "[[ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]] [10775.] \n",
      "\n",
      "[[ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]] [10776.] \n",
      "\n",
      "[[ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]] [10777.] \n",
      "\n",
      "[[ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]] [10778.] \n",
      "\n",
      "[[ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]] [10779.] \n",
      "\n",
      "[[ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]] [10780.] \n",
      "\n",
      "[[ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]] [10781.] \n",
      "\n",
      "[[ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]] [10782.] \n",
      "\n",
      "[[ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]] [10783.] \n",
      "\n",
      "[[ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]] [10784.] \n",
      "\n",
      "[[ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]] [10785.] \n",
      "\n",
      "[[ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]] [10786.] \n",
      "\n",
      "[[ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]] [10787.] \n",
      "\n",
      "[[ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]\n",
      " [ 786. 2786. 4786. 6786. 8786.]] [10788.] \n",
      "\n",
      "[[ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]\n",
      " [ 787. 2787. 4787. 6787. 8787.]] [10789.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-1], test_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator using `yield`\n",
    "Instead of using `return` statement, we can use `yield` statement, which is more memory efficient because it does not ruturn the whole array i.e. `train_x_batches` at once rather it gives one batch at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    " \n",
    "# Instead of function we construct a class which is initialized with almost same arguments as were used in previous function.\n",
    "class batch_generator(object):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param batch_size: `int`, batch size to be used\n",
    "    :param args: a dictionary containing values of parameters depending upon method used.\n",
    "    :param method: str, default is 'many_to_one', if many_to_one, then following keys are expected in \n",
    "                   dictionary args.\n",
    "            :lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "            :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "            :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "            :trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "            :norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                         and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "            :batch_size:\n",
    "            :step: step size in input data\n",
    "            :min_ind: starting point from `data`\n",
    "            :max_ind: end point from `data`\n",
    "            :future_y_val: number of values to predict\n",
    "            \n",
    "    :param verbose: `boolean`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, batch_size, args, method='many_to_one', verbose=True):\n",
    "        \n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.args = args\n",
    "        self.method=method\n",
    "        self.verbose=verbose\n",
    "        self.ignoriert_am_anfang=None\n",
    "        self.ignoriert_am_ende = None\n",
    "        self.no_of_batches = None\n",
    "    \n",
    "    \n",
    "    def many_to_one(self):\n",
    "    \n",
    "        many_to_one_args = {'lookback': 'required',\n",
    "                            'in_features': 'required',\n",
    "                            'out_features': 'required',\n",
    "                            'min_ind': 'required',\n",
    "                            'max_ind': 'required',\n",
    "                            'future_y_val': 'required',\n",
    "                            'step': 1,\n",
    "                            'norm': None,\n",
    "                            'trim_last_batch':True}\n",
    "\n",
    "        for k,v in many_to_one_args.items():\n",
    "            if v=='required':\n",
    "                if k not in self.args:\n",
    "                    raise ValueError('for {} method, value of {} is required'.format(method, k))\n",
    "                else:\n",
    "                    many_to_one_args[k] = self.args[k]\n",
    "            else:\n",
    "                if k in self.args:\n",
    "                    many_to_one_args[k] = self.args[k]\n",
    "\n",
    "        lookback = many_to_one_args['lookback']\n",
    "        in_features = many_to_one_args['in_features']\n",
    "        out_features = many_to_one_args['out_features']\n",
    "        min_ind = many_to_one_args['min_ind']\n",
    "        max_ind = many_to_one_args['max_ind']\n",
    "        future_y_val = many_to_one_args['future_y_val']\n",
    "        step = many_to_one_args['step']\n",
    "        norm = many_to_one_args['norm']\n",
    "        trim_last_batch = many_to_one_args['trim_last_batch']\n",
    "\n",
    "        # selecting the data of interest for x and y    \n",
    "        X = self.data[min_ind:max_ind, 0:in_features]\n",
    "        Y = self.data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "\n",
    "        if norm is not None:\n",
    "            x_scaler = norm['x_scaler']\n",
    "            y_scaler = norm['y_scaler']\n",
    "            X = x_scaler.fit_transform(X)\n",
    "            Y = y_scaler.fit_transform(Y)\n",
    "\n",
    "        # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "        x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "        y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "\n",
    "        # creating windows from X data\n",
    "        st = lookback*step - step # starting point of sampling from data\n",
    "        for j in range(st, X.shape[0]-lookback):\n",
    "            en = j - lookback*step\n",
    "            indices = np.arange(j, en, -step)\n",
    "            ind = np.flip(indices)\n",
    "            x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "        # creating windows from Y data\n",
    "        for i in range(0, Y.shape[0]-lookback):\n",
    "            y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"removing trailing nans\"\"\"\n",
    "        first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "        y_wins = y_wins[0:first_nan_at_end,:]\n",
    "        x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "        \"\"\"removing nans from start\"\"\"\n",
    "        y_val = st-lookback + future_y_val\n",
    "        if st>0:\n",
    "            x_wins = x_wins[st:,:]\n",
    "            y_wins = y_wins[y_val:,:]    \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "            print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "              .format(st, X.shape[0]-first_nan_at_end))\n",
    "        self.ignoriert_am_anfang = st\n",
    "        self.ignoriert_am_ende = X.shape[0]-first_nan_at_end\n",
    "\n",
    "        pot_samples = x_wins.shape[0]\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "        residue = pot_samples % self.batch_size\n",
    "        if self.verbose:\n",
    "            print('\\nresidue is {} '.format(residue))\n",
    "        self.residue = residue\n",
    "\n",
    "        samples = pot_samples - residue\n",
    "        if self.verbose:\n",
    "            print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "        interval = np.arange(0, samples + self.batch_size, self.batch_size)\n",
    "        if self.verbose:\n",
    "            print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "        interval = np.append(interval, pot_samples)\n",
    "        if self.verbose:\n",
    "            print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "        if trim_last_batch:\n",
    "            no_of_batches = len(interval)-2\n",
    "        else:\n",
    "            no_of_batches = len(interval) - 1 \n",
    "\n",
    "        print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "        self.no_of_batches = no_of_batches\n",
    "\n",
    "        # code for generator\n",
    "        gen_i = 1\n",
    "        while 1:\n",
    "\n",
    "            for b in range(no_of_batches):\n",
    "                st = interval[b]\n",
    "                en = interval[b + 1]\n",
    "                x_batch = x_wins[st:en, :, :]\n",
    "                y_batch = y_wins[st:en]\n",
    "\n",
    "                gen_i +=1\n",
    "\n",
    "                yield x_batch, y_batch\n",
    "            \n",
    "\n",
    "\n",
    "_lookback=2  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': st_ind,\n",
    "            'max_ind': end_ind,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': train_x_scaler, 'y_scaler': train_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "train_generator = batch_generator(data, _batch_size, train_args) \n",
    "train_gen = train_generator.many_to_one()\n",
    "\n",
    "val_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': 600,\n",
    "            'max_ind': 800,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': val_x_scaler, 'y_scaler': val_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "val_generator = batch_generator(data, _batch_size, val_args, verbose=False) \n",
    "val_gen = val_generator.many_to_one()\n",
    "\n",
    "test_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': 800,\n",
    "            'max_ind': 1000,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': test_x_scaler, 'y_scaler': test_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "test_generator = batch_generator(data, _batch_size, test_args)\n",
    "test_gen = test_generator.many_to_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data generator\n",
    "The values are normalized between 0 and 1. We can run the next cell multiple times and each time a different batch is printed. If we want to see exact values, we can turn-off normalization by setting `norm` value to None in above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(val_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(test_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
