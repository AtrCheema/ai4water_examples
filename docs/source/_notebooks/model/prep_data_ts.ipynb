{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook describes how to prepare data for RNN/LSTM for timeseries prediction problem especially for Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Srtqrlwr09Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "Create a data which is supposed to represent a timeseries prediction problem. The data has 6 columns and 1000 rows. The first five columns are supposed to be input and the last column is supposed to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  2000  4000  6000  8000 10000]\n",
      " [    1  2001  4001  6001  8001 10001]\n",
      " [    2  2002  4002  6002  8002 10002]\n",
      " [    3  2003  4003  6003  8003 10003]\n",
      " [    4  2004  4004  6004  8004 10004]\n",
      " [    5  2005  4005  6005  8005 10005]\n",
      " [    6  2006  4006  6006  8006 10006]\n",
      " [    7  2007  4007  6007  8007 10007]\n",
      " [    8  2008  4008  6008  8008 10008]\n",
      " [    9  2009  4009  6009  8009 10009]\n",
      " [   10  2010  4010  6010  8010 10010]\n",
      " [   11  2011  4011  6011  8011 10011]\n",
      " [   12  2012  4012  6012  8012 10012]\n",
      " [   13  2013  4013  6013  8013 10013]\n",
      " [   14  2014  4014  6014  8014 10014]\n",
      " [   15  2015  4015  6015  8015 10015]\n",
      " [   16  2016  4016  6016  8016 10016]\n",
      " [   17  2017  4017  6017  8017 10017]\n",
      " [   18  2018  4018  6018  8018 10018]\n",
      " [   19  2019  4019  6019  8019 10019]]\n",
      "\n",
      " (2000, 6) \n",
      "\n",
      "[[ 1980  3980  5980  7980  9980 11980]\n",
      " [ 1981  3981  5981  7981  9981 11981]\n",
      " [ 1982  3982  5982  7982  9982 11982]\n",
      " [ 1983  3983  5983  7983  9983 11983]\n",
      " [ 1984  3984  5984  7984  9984 11984]\n",
      " [ 1985  3985  5985  7985  9985 11985]\n",
      " [ 1986  3986  5986  7986  9986 11986]\n",
      " [ 1987  3987  5987  7987  9987 11987]\n",
      " [ 1988  3988  5988  7988  9988 11988]\n",
      " [ 1989  3989  5989  7989  9989 11989]\n",
      " [ 1990  3990  5990  7990  9990 11990]\n",
      " [ 1991  3991  5991  7991  9991 11991]\n",
      " [ 1992  3992  5992  7992  9992 11992]\n",
      " [ 1993  3993  5993  7993  9993 11993]\n",
      " [ 1994  3994  5994  7994  9994 11994]\n",
      " [ 1995  3995  5995  7995  9995 11995]\n",
      " [ 1996  3996  5996  7996  9996 11996]\n",
      " [ 1997  3997  5997  7997  9997 11997]\n",
      " [ 1998  3998  5998  7998  9998 11998]\n",
      " [ 1999  3999  5999  7999  9999 11999]]\n"
     ]
    }
   ],
   "source": [
    "rows = 2000\n",
    "cols = 6\n",
    "data = np.arange(int(rows*cols)).reshape(-1,rows).transpose()\n",
    "print(data[0:20])  \n",
    "print('\\n {} \\n'.format(data.shape))\n",
    "print(data[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (581, 7, 5) \n",
      "shape of y data: (587, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 581\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 576\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 581] \n",
      "\n",
      "Number of batches are 36 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n"
     ]
    }
   ],
   "source": [
    "def first_nan_from_end(ar):\n",
    "    \"\"\" \n",
    "    This function finds index for first nan from the group which is present at the end of input array `ar`.\n",
    "    Some examples are below\n",
    "    [np.nan, np.nan, 0, 2, 3, 0, 3, np.nan, np.nan, np.nan, np.nan] >> 7\n",
    "    [np.nan, np.nan, 1, 2, 3, 0, np.nan, np.nan, np.nan] >> 6\n",
    "    [0, 2, 3, 0, 3] >> 5\n",
    "    [np.nan, np.nan, 0,2,3,0,3] >> 7    \n",
    "    \"\"\"\n",
    "    last_non_zero=0\n",
    "    \n",
    "    for idx, val in enumerate(ar[::-1]):  # first find first non-nan value starting from last\n",
    "        if ~np.isnan(val): # val >= 0:\n",
    "            last_non_zero = idx\n",
    "            break\n",
    "    return ar.shape[0] - last_non_zero    \n",
    "    \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=None, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    # normalizting both x and y data\n",
    "    if norm:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)        \n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "    \n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step                 # starting point of sampling from data may not start from 0 \n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans or nans which are present at the end part of array\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:                              # if some values from start were skipped, we need to remove nans from those places\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]\n",
    "\n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]  # ptential samples\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    if residue > 0:\n",
    "        interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "    \n",
    "    # The last batch may not fewer data as other batches. We can skip that incomplete batch.\n",
    "    # This can be useful if want to save our batches in a list i.e. if 'x_batches' is a list.\n",
    "    if trim_last_batch:   #TODO this must be obligatory when saving batches in numpy array\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "        \n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "\n",
    "    # container for batches\n",
    "    x_batches = np.full((no_of_batches, batch_size, lookback, in_features), np.nan)\n",
    "    y_batches = np.full((no_of_batches, batch_size, out_features), np.nan)\n",
    "\n",
    "\n",
    "    for b in range(no_of_batches):\n",
    "        st = interval[b]\n",
    "        en = interval[b + 1]\n",
    "        an_x_batch = x_wins[st:en, :, :]\n",
    "        x_batches[b] = an_x_batch\n",
    "       # y_batches[b] = y_wins[st:en]\n",
    "        y_batches[b] = y_wins[st+1:en+1]\n",
    "\n",
    "\n",
    "    print('\\nshape of batches for:')\n",
    "    print('x_data ', ' y_data')\n",
    "    for i,j in zip(x_batches, y_batches):\n",
    "        ishp, jshp = None, None\n",
    "        if isinstance(i, np.ndarray):\n",
    "            ishp = i.shape\n",
    "        if isinstance(j, np.ndarray):\n",
    "            jshp = j.shape\n",
    "        print(ishp, jshp)\n",
    "    \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=7  # sequence length\n",
    "input_features = 5  # number of columns in dataset to be used as input\n",
    "output_features = 1 # number of column to be used as output\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_x_batches, train_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    trim_last_batch = True)            \n",
    "\n",
    "test_x_batches, test_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                                    min_ind = 600,\n",
    "                                    max_ind = 800,\n",
    "                                    future_y_val = t_plus_ith_val,\n",
    "                                    trim_last_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first train batch\n",
    "We can look at first training batch which consist of 16 windows where each window has shape [7, 5]. One training window is fed at one time step and each window has correspoding prediction.\n",
    "Also look where each window starts and ends and the `y` values for each window.\n",
    "Values in rows of windows are not continuous because o f `input_stepsize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 2000. 4000. 6000. 8000.]\n",
      " [   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]] [10014.] \n",
      "\n",
      "[[   1. 2001. 4001. 6001. 8001.]\n",
      " [   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]] [10015.] \n",
      "\n",
      "[[   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]] [10016.] \n",
      "\n",
      "[[   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]] [10017.] \n",
      "\n",
      "[[   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]] [10018.] \n",
      "\n",
      "[[   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]] [10019.] \n",
      "\n",
      "[[   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]] [10020.] \n",
      "\n",
      "[[   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]] [10021.] \n",
      "\n",
      "[[   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]] [10022.] \n",
      "\n",
      "[[   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]] [10023.] \n",
      "\n",
      "[[  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]] [10024.] \n",
      "\n",
      "[[  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]] [10025.] \n",
      "\n",
      "[[  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]] [10026.] \n",
      "\n",
      "[[  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]] [10027.] \n",
      "\n",
      "[[  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]] [10028.] \n",
      "\n",
      "[[  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]] [10029.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[0], train_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to skip certain values from start because of `lookup` size\n",
    "### second train batch\n",
    "Windows in second training batch start from exactly where the windows in first ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]] [10030.] \n",
      "\n",
      "[[  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]] [10031.] \n",
      "\n",
      "[[  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]] [10032.] \n",
      "\n",
      "[[  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]] [10033.] \n",
      "\n",
      "[[  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]] [10034.] \n",
      "\n",
      "[[  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]] [10035.] \n",
      "\n",
      "[[  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]] [10036.] \n",
      "\n",
      "[[  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]] [10037.] \n",
      "\n",
      "[[  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]] [10038.] \n",
      "\n",
      "[[  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]] [10039.] \n",
      "\n",
      "[[  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]] [10040.] \n",
      "\n",
      "[[  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]] [10041.] \n",
      "\n",
      "[[  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]] [10042.] \n",
      "\n",
      "[[  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]] [10043.] \n",
      "\n",
      "[[  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]\n",
      " [  42. 2042. 4042. 6042. 8042.]] [10044.] \n",
      "\n",
      "[[  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]\n",
      " [  43. 2043. 4043. 6043. 8043.]] [10045.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[1], train_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last train batch\n",
    "Some values from end are also missing because we skipped the last batch which did not have the shape of [16, 7, 5]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 560. 2560. 4560. 6560. 8560.]\n",
      " [ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]] [10574.] \n",
      "\n",
      "[[ 561. 2561. 4561. 6561. 8561.]\n",
      " [ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]] [10575.] \n",
      "\n",
      "[[ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]] [10576.] \n",
      "\n",
      "[[ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]] [10577.] \n",
      "\n",
      "[[ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]] [10578.] \n",
      "\n",
      "[[ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]] [10579.] \n",
      "\n",
      "[[ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]] [10580.] \n",
      "\n",
      "[[ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]] [10581.] \n",
      "\n",
      "[[ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]] [10582.] \n",
      "\n",
      "[[ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]] [10583.] \n",
      "\n",
      "[[ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]] [10584.] \n",
      "\n",
      "[[ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]] [10585.] \n",
      "\n",
      "[[ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]] [10586.] \n",
      "\n",
      "[[ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]] [10587.] \n",
      "\n",
      "[[ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]\n",
      " [ 586. 2586. 4586. 6586. 8586.]] [10588.] \n",
      "\n",
      "[[ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]\n",
      " [ 587. 2587. 4587. 6587. 8587.]] [10589.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[-1], train_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first test batch\n",
    "The start of test data depends upon the value of `min_ind` and `max_ind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600. 2600. 4600. 6600. 8600.]\n",
      " [ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]] [10614.] \n",
      "\n",
      "[[ 601. 2601. 4601. 6601. 8601.]\n",
      " [ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]] [10615.] \n",
      "\n",
      "[[ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]] [10616.] \n",
      "\n",
      "[[ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]] [10617.] \n",
      "\n",
      "[[ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]] [10618.] \n",
      "\n",
      "[[ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]] [10619.] \n",
      "\n",
      "[[ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]] [10620.] \n",
      "\n",
      "[[ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]] [10621.] \n",
      "\n",
      "[[ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]] [10622.] \n",
      "\n",
      "[[ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]] [10623.] \n",
      "\n",
      "[[ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]] [10624.] \n",
      "\n",
      "[[ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]] [10625.] \n",
      "\n",
      "[[ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]] [10626.] \n",
      "\n",
      "[[ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]] [10627.] \n",
      "\n",
      "[[ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]] [10628.] \n",
      "\n",
      "[[ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]] [10629.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[0], test_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]] [10630.] \n",
      "\n",
      "[[ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]] [10631.] \n",
      "\n",
      "[[ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]] [10632.] \n",
      "\n",
      "[[ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]] [10633.] \n",
      "\n",
      "[[ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]] [10634.] \n",
      "\n",
      "[[ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]] [10635.] \n",
      "\n",
      "[[ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]] [10636.] \n",
      "\n",
      "[[ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]] [10637.] \n",
      "\n",
      "[[ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]] [10638.] \n",
      "\n",
      "[[ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]] [10639.] \n",
      "\n",
      "[[ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]] [10640.] \n",
      "\n",
      "[[ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]] [10641.] \n",
      "\n",
      "[[ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]] [10642.] \n",
      "\n",
      "[[ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]] [10643.] \n",
      "\n",
      "[[ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]\n",
      " [ 642. 2642. 4642. 6642. 8642.]] [10644.] \n",
      "\n",
      "[[ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]\n",
      " [ 643. 2643. 4643. 6643. 8643.]] [10645.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[1], test_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 744. 2744. 4744. 6744. 8744.]\n",
      " [ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]] [10758.] \n",
      "\n",
      "[[ 745. 2745. 4745. 6745. 8745.]\n",
      " [ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]] [10759.] \n",
      "\n",
      "[[ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]] [10760.] \n",
      "\n",
      "[[ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]] [10761.] \n",
      "\n",
      "[[ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]] [10762.] \n",
      "\n",
      "[[ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]] [10763.] \n",
      "\n",
      "[[ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]] [10764.] \n",
      "\n",
      "[[ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]] [10765.] \n",
      "\n",
      "[[ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]] [10766.] \n",
      "\n",
      "[[ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]] [10767.] \n",
      "\n",
      "[[ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]] [10768.] \n",
      "\n",
      "[[ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]] [10769.] \n",
      "\n",
      "[[ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]] [10770.] \n",
      "\n",
      "[[ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]] [10771.] \n",
      "\n",
      "[[ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]] [10772.] \n",
      "\n",
      "[[ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]] [10773.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-2], test_y_batches[-2]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]] [10774.] \n",
      "\n",
      "[[ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]] [10775.] \n",
      "\n",
      "[[ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]] [10776.] \n",
      "\n",
      "[[ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]] [10777.] \n",
      "\n",
      "[[ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]] [10778.] \n",
      "\n",
      "[[ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]] [10779.] \n",
      "\n",
      "[[ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]] [10780.] \n",
      "\n",
      "[[ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]] [10781.] \n",
      "\n",
      "[[ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]] [10782.] \n",
      "\n",
      "[[ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]] [10783.] \n",
      "\n",
      "[[ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]] [10784.] \n",
      "\n",
      "[[ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]] [10785.] \n",
      "\n",
      "[[ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]] [10786.] \n",
      "\n",
      "[[ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]] [10787.] \n",
      "\n",
      "[[ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]\n",
      " [ 786. 2786. 4786. 6786. 8786.]] [10788.] \n",
      "\n",
      "[[ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]\n",
      " [ 787. 2787. 4787. 6787. 8787.]] [10789.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-1], test_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Generator using `yield`\n",
    "Instead of using `return` statement, we can use `yield` statement, which is more memory efficient because it does not ruturn the whole array i.e. `train_x_batches` at once rather it gives one batch at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    " \n",
    "# Instead of function we construct a class which is initialized with almost same arguments as were used in previous function.\n",
    "class batch_generator(object):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param batch_size: `int`, batch size to be used\n",
    "    :param args: a dictionary containing values of parameters depending upon method used.\n",
    "    :param method: str, default is 'many_to_one', if many_to_one, then following keys are expected in \n",
    "                   dictionary args.\n",
    "            :lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "            :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "            :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "            :trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "            :norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                         and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "            :batch_size:\n",
    "            :step: step size in input data\n",
    "            :min_ind: starting point from `data`\n",
    "            :max_ind: end point from `data`\n",
    "            :future_y_val: number of values to predict\n",
    "            \n",
    "    :param verbose: `boolean`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, batch_size, args, method='many_to_one', verbose=True):\n",
    "        \n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.args = args\n",
    "        self.method=method\n",
    "        self.verbose=verbose\n",
    "        self.ignoriert_am_anfang=None\n",
    "        self.ignoriert_am_ende = None\n",
    "        self.no_of_batches = None\n",
    "    \n",
    "    \n",
    "    def many_to_one(self):\n",
    "    \n",
    "        many_to_one_args = {'lookback': 'required',\n",
    "                            'in_features': 'required',\n",
    "                            'out_features': 'required',\n",
    "                            'min_ind': 'required',\n",
    "                            'max_ind': 'required',\n",
    "                            'future_y_val': 'required',\n",
    "                            'step': 1,\n",
    "                            'norm': None,\n",
    "                            'trim_last_batch':True}\n",
    "\n",
    "        for k,v in many_to_one_args.items():\n",
    "            if v=='required':\n",
    "                if k not in self.args:\n",
    "                    raise ValueError('for {} method, value of {} is required'.format(method, k))\n",
    "                else:\n",
    "                    many_to_one_args[k] = self.args[k]\n",
    "            else:\n",
    "                if k in self.args:\n",
    "                    many_to_one_args[k] = self.args[k]\n",
    "\n",
    "        lookback = many_to_one_args['lookback']\n",
    "        in_features = many_to_one_args['in_features']\n",
    "        out_features = many_to_one_args['out_features']\n",
    "        min_ind = many_to_one_args['min_ind']\n",
    "        max_ind = many_to_one_args['max_ind']\n",
    "        future_y_val = many_to_one_args['future_y_val']\n",
    "        step = many_to_one_args['step']\n",
    "        norm = many_to_one_args['norm']\n",
    "        trim_last_batch = many_to_one_args['trim_last_batch']\n",
    "\n",
    "        # selecting the data of interest for x and y    \n",
    "        X = self.data[min_ind:max_ind, 0:in_features]\n",
    "        Y = self.data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "\n",
    "        if norm is not None:\n",
    "            x_scaler = norm['x_scaler']\n",
    "            y_scaler = norm['y_scaler']\n",
    "            X = x_scaler.fit_transform(X)\n",
    "            Y = y_scaler.fit_transform(Y)\n",
    "\n",
    "        # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "        x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "        y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "\n",
    "        # creating windows from X data\n",
    "        st = lookback*step - step # starting point of sampling from data\n",
    "        for j in range(st, X.shape[0]-lookback):\n",
    "            en = j - lookback*step\n",
    "            indices = np.arange(j, en, -step)\n",
    "            ind = np.flip(indices)\n",
    "            x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "        # creating windows from Y data\n",
    "        for i in range(0, Y.shape[0]-lookback):\n",
    "            y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"removing trailing nans\"\"\"\n",
    "        first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "        y_wins = y_wins[0:first_nan_at_end,:]\n",
    "        x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "        \"\"\"removing nans from start\"\"\"\n",
    "        y_val = st-lookback + future_y_val\n",
    "        if st>0:\n",
    "            x_wins = x_wins[st:,:]\n",
    "            y_wins = y_wins[y_val:,:]    \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "            print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "              .format(st, X.shape[0]-first_nan_at_end))\n",
    "        self.ignoriert_am_anfang = st\n",
    "        self.ignoriert_am_ende = X.shape[0]-first_nan_at_end\n",
    "\n",
    "        pot_samples = x_wins.shape[0]\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "        residue = pot_samples % self.batch_size\n",
    "        if self.verbose:\n",
    "            print('\\nresidue is {} '.format(residue))\n",
    "        self.residue = residue\n",
    "\n",
    "        samples = pot_samples - residue\n",
    "        if self.verbose:\n",
    "            print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "        interval = np.arange(0, samples + self.batch_size, self.batch_size)\n",
    "        if self.verbose:\n",
    "            print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "        interval = np.append(interval, pot_samples)\n",
    "        if self.verbose:\n",
    "            print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "        if trim_last_batch:\n",
    "            no_of_batches = len(interval)-2\n",
    "        else:\n",
    "            no_of_batches = len(interval) - 1 \n",
    "\n",
    "        print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "        self.no_of_batches = no_of_batches\n",
    "\n",
    "        # code for generator\n",
    "        gen_i = 1\n",
    "        while 1:\n",
    "\n",
    "            for b in range(no_of_batches):\n",
    "                st = interval[b]\n",
    "                en = interval[b + 1]\n",
    "                x_batch = x_wins[st:en, :, :]\n",
    "                y_batch = y_wins[st:en]\n",
    "\n",
    "                gen_i +=1\n",
    "\n",
    "                yield x_batch, y_batch\n",
    "            \n",
    "\n",
    "\n",
    "_lookback=2  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': st_ind,\n",
    "            'max_ind': end_ind,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': train_x_scaler, 'y_scaler': train_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "train_generator = batch_generator(data, _batch_size, train_args) \n",
    "train_gen = train_generator.many_to_one()\n",
    "\n",
    "val_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': 600,\n",
    "            'max_ind': 800,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': val_x_scaler, 'y_scaler': val_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "val_generator = batch_generator(data, _batch_size, val_args, verbose=False) \n",
    "val_gen = val_generator.many_to_one()\n",
    "\n",
    "test_args = {'lookback': _lookback,\n",
    "            'in_features': input_features,\n",
    "            'out_features': output_features,\n",
    "            'min_ind': 800,\n",
    "            'max_ind': 1000,\n",
    "            'future_y_val': t_plus_ith_val,\n",
    "            'step': input_stepsize,\n",
    "            'norm': {'x_scaler': test_x_scaler, 'y_scaler': test_y_scaler},\n",
    "            'trim_last_batch':True}\n",
    "\n",
    "test_generator = batch_generator(data, _batch_size, test_args)\n",
    "test_gen = test_generator.many_to_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data generator\n",
    "The values are normalized between 0 and 1. We can run the next cell multiple times and each time a different batch is printed. If we want to see exact values, we can turn-off normalization by setting `norm` value to None in above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(val_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(test_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model using keras API\n",
    "We can test whether our data creation is correct or not by using it in a simple ML model using Keras API. The following model is not optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "tensorflow.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1229 21:12:21.588983 46212 deprecation.py:506] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "W1229 21:12:22.104603 46212 deprecation.py:323] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "shape of x data: (596, 2, 5) \n",
      "shape of y data: (597, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 596\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 592\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592 596] \n",
      "\n",
      "Number of batches are 37 \n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.0532 - val_loss: 0.0587\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0463 - val_loss: 0.0459\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0372 - val_loss: 0.0536\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0291 - val_loss: 0.0114\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0272 - val_loss: 0.0570\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.0259 - val_loss: 0.0257\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0241 - val_loss: 0.0595\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0225 - val_loss: 0.0345\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0203 - val_loss: 0.0434\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0188 - val_loss: 0.0232\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0178 - val_loss: 0.0499\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0170 - val_loss: 0.0146\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0159 - val_loss: 0.0646\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0154 - val_loss: 0.0190\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0148 - val_loss: 0.0517\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0142 - val_loss: 0.0416\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0144 - val_loss: 0.0371\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0138 - val_loss: 0.0420\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0136 - val_loss: 0.0305\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(64, input_shape=(_lookback, input_features), dropout=0.1, recurrent_dropout=0.5,))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                            steps_per_epoch=500,\n",
    "                            epochs=20,\n",
    "                            validation_data=val_gen,\n",
    "                            validation_steps=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/sample\n",
      "[10819.] [10816.539]\n",
      "[10820.] [10817.387]\n",
      "[10821.] [10818.236]\n",
      "[10822.] [10819.092]\n",
      "[10823.] [10819.949]\n",
      "[10824.] [10820.812]\n",
      "[10825.] [10821.678]\n",
      "[10826.] [10822.548]\n",
      "[10827.] [10823.422]\n",
      "[10828.] [10824.299]\n",
      "[10829.] [10825.179]\n",
      "[10830.] [10826.063]\n",
      "[10831.] [10826.951]\n",
      "[10832.] [10827.843]\n",
      "[10833.] [10828.737]\n",
      "[10834.] [10829.636]\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = next(test_gen)\n",
    "test_prediction = model.predict(test_x, 1, verbose=True)\n",
    "test_predict = test_y_scaler.inverse_transform(test_prediction)\n",
    "true = test_y_scaler.inverse_transform(test_y)\n",
    "for t,p in zip(true, test_predict):\n",
    "    print(t,p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
